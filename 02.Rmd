# Fantasy Sports: Helping Players Up Their Game by Grading Pundits on Reliability {#prediction-accuracy}

#### Keywords

Natural language processing, crowd-sourcing, prediction analysis

## Sports opinions abound, reliability metrics are scarce

The fantasy sports industry now represents a [$36 billion market](http://fsta.org/research/industry-demographics/) in the U.S. and Canada, with roughly a tenth of that in publishing. Much of the published material contains predictions by pundits about how various players will perform in the coming week and season. Players looking for advice have no shortage of places to turn, but they also have few ways to know which pundit has a good track record of making correct predictions. 

Similar to the “fake news” problem in the general news space, fantasy players lack tools to gauge the accuracy and reliability of what they are reading -- in this case about future events, rather than current events. Fantasy sports players stand to make more money if they play their games better, meaning they would have an incentive to pay for a tool that helps them improve their information intake. 

## Scrape predictions, crowdsource their accuracy

Currently, fantasy players do have some limited ways to gauge the accuracy of forecasters. Those pundits who make systematic, numerical-only projections across a full set of players can be graded relatively simply -- and there are a few (self-interested) [services out there](http://fantasyfootballanalytics.net/2016/03/best-fantasy-football-projections-2016-update.html) doing so. Many predictions, however, do not fit this mold but are less systematic, focused on just one player, or more qualitative in nature. These predictions are often more interesting as they are offered in the course of conversations between pundits or explained in engaging writeups rather than data tables.
 
I am proposing a way to measure the reliability of pundits making these less structured forecasts:
1) Use natural language processing to scan sports websites and broadcast transcripts to search for predictions. A prediction must include: a person making the prediction, a timeframe, a binary outcome. 
2) Show the predictions immediately to players, along with any reliability measure of the pundit. 
3) When each prediction comes due, let players weigh in whether the prediction was correct (binary: true / false). If a critical mass of players agree, rate the prediction as true or false and update the pundit’s reliability score. 

For example, John Doe is a pundit who has previously made one correct prediction and four incorrect predictions. He writes that Tom Brady will throw no touchdowns in next Sunday’s game. The prediction is saved to a database and shown to fantasy players who have picked Tom Brady as a player, along with a note that John Doe is right 20% of the time. After next week’s game, players are shown the prediction again to rate it true or false. Ten people rate the prediction true and so John Doe’s reliability rating goes to 33%.

## Clean up the information ecosystem, for sports and beyond

There are many areas of life where people make predictions. Fantasy sports provides one of the easiest areas to determine pundit reliability given the abundance of predictions, their short time intervals, and the motivation of consumers to care about reliability. Personally, I’m not all that interested in sports. I am interested in improving the information landscape. I could see the tactics developed here eventually migrated over to political punditry, investing forecasts, business risk intelligence, and government intelligence. 

This model only works with predictions that can be treated as binary, so that leaves out a good deal of nuanced or quantitative forecasting. The model will have to decide how to treat conditionals such as “The U.S. economy <b>could</b> grow by 4 percent this year.” (I would argue “could” should be treated like a “will.”) 

An additional complication: If this tool grew popular, pundits might begin hedging their predictions in ways that make it harder to measure, or they may start making more “no-brainer” predictions that don’t add value. 

But, overall, the negatives are outweighed by the promise of greater accountability around prognostication. Pundits who are consistently wrong will finally be called out and weeded out of the information ecosystem. This is a particularly attractive outcome in the political punditry arena. The fools with megaphones who claimed that U.S. troops would be greeted as liberators in Iraq and Hillary Clinton would win in a landslide could be ushered out of the conversation.
