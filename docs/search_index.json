[
["index.html", "W201 Portfolio Welcome! About the author", " W201 Portfolio Ben Arnoldy MIDS Fall 2017 Welcome! I am interested in the intersection between data science and mass media. Written word communication represents a significant chunk of the world’s amassed data. Located in that data could be the answers to some of the enigmas of our age, including how to persuade climate deniers, how to eradicate fake news, and how to determine who’s a reliable pundit. Figure .: Pitch 0.0.1 Abstract: Tracking Pundit Reliability in Fantasy Sports Research has found that the majority of experts making forecasts had a track record no better than random chance – but a minority of pundits were reliably accurate. Identifying those reliable pundits in a relevant field would give an information edge to anyone making bets on future events. I propose a way to build a tool to measure forecaster reliability for the fantasy sports industry. The tool would use natural language processing to extract measurable predictions from the vast output of fantasy sports publishing. As predictions come due, each would be judged for accuracy through crowdsourcing and the pundit’s reliability rating would be subsequently moved upward or downward. Lessons learned in the easier arena of fantasy sports could then be applied to more difficult but more consequential arenas such as business risk intelligence, politics, and government intelligence. 0.0.2 Abstract: Winning Public Opinion by Measuring Minds Changed Organizations tasked with changing public opinion do not have fast or cheap ways to figure out which messages work best. Nor can they easily measure success: A blog post might go viral, but that doesn’t mean it changed anyone’s mind. I propose a new mechanism for discovering the most persuasive messaging by searching and analyzing online discussions on specific topics. An algorithm will look at individual commenters over the span of years and will conduct sentiment analysis on their posts to find any change of mind. The algorithm will then look for new words or phrases used at the moment of change and trace them back to a source article or comment, in this way identifying persuasive texts. Organizations can use the real-world, real-time trends found in these persuasive texts to replace expensive focus group testing and become better at communicating outside their bubbles. About the author Ben Arnoldy is managing editor at Earthjustice in San Francisco and a graduate student in UC Berkeley’s MIDS program. He spent his first career in journalism as a reporter, foreign correspondent, editor, producer, and publishing product manager. See his linkedin profile for more. Updated: 2017-11-29 "],
["prediction-accuracy.html", "1 Helping Fantasy Sports Players Up Their Game by Grading Pundits on Reliability Sports opinions abound, reliability metrics are scarce Scrape predictions, crowdsource their accuracy Clean up the information ecosystem, for sports and beyond", " 1 Helping Fantasy Sports Players Up Their Game by Grading Pundits on Reliability Keywords Natural language processing, crowd-sourcing, prediction analysis Sports opinions abound, reliability metrics are scarce My friend Mark told me recently how he had shelled out $500 for a special scouting report to help improve his Fantasy Football draft picks. Mark is among the 60 million people in North America who are playing fantasy sports this year. We spend $36 billion each year on fantasy sports, with roughly a tenth of that going to published materials like what Mark bought. Much of that material contains predictions by pundits about how various athletes will perform in the coming week and season. Mark chose the $500 pundit based on word of mouth and a sense that the information must be good if it’s $500. However, Mark is doing worse this season taking the pundit’s advice than last season. It’s easy to laugh and dismiss all pundits as blowhards making guesses like the rest of us. But research says differently. While most forecasters are like the one Mark chose, there is a minority of forecasters who really do outperform dart-throwing chimps. Could we measure the track record of correct predictions by various pundits so that fantasy sports gamers can focus their reading on the most reliable forecasters, and ultimately, play better? Scrape predictions, crowdsource their accuracy Currently, fantasy gamers do have some limited ways to gauge the accuracy of forecasters. Those pundits who make systematic, numerical-only projections across a full set of athletes can be graded relatively simply – and there are a few (self-interested) services out there doing so. Many predictions, however, do not fit this mold but are less systematic, focused on just one athlete, or more qualitative in nature. These predictions are often more interesting as they are offered in the course of conversations between pundits or explained in engaging writeups rather than data tables. I am proposing a way to measure the reliability of pundits making these less structured forecasts: 1) Use natural language processing to scan sports websites and broadcast transcripts to search for predictions. A prediction must include: a person making the prediction, a timeframe, and a binary outcome. 2) Show the predictions immediately to gamers, along with any reliability measure of the pundit. 3) When each prediction comes due, let gamers weigh in whether the prediction was correct. If a critical mass of gamers agree, rate the prediction as true or false and update the pundit’s reliability score. For example, John Doe is a pundit who has previously made one correct prediction and four incorrect predictions. He writes that Tom Brady will throw no touchdowns in next Sunday’s game. The prediction is saved to a database and shown to fantasy sports gamers who have Brady on their team, along with a note that John Doe is right 20% of the time. After next week’s game, gamers are shown the prediction again to rate it. Ten people rate the prediction true, no one dissents, and so John Doe’s reliability rating goes to 33%. Clean up the information ecosystem, for sports and beyond There are many areas of life where people make predictions. Fantasy sports provides one of the easiest areas to determine pundit reliability given the abundance of predictions, their short time intervals, and the motivation of consumers like Mark to care about reliability. But the problem remains tricky, even in this space. This model only works with predictions that can be treated as binary, so that leaves out a good deal of nuanced or quantitative forecasting. The model will have to decide how to treat conditionals such as “The U.S. economy could grow by 4% this year.” (I would argue “could” should be treated like a “will.”) An additional complication: If this tool grew popular, pundits might begin hedging their predictions in ways that make it harder to measure, or they may start making more “no-brainer” predictions that don’t add value. The tool may have to adapt in ways to take into account the perceived difficulty of the prediction. However, even if flaws emerged, the tool’s usefulness could be easily measured: Take a random sample of fantasy sports gamers and a control group and compare their overall scores. If the tool is shown to work for fantasy sports, it could be adapted for more consequential arenas like business risk intelligence, politics, and even government intelligence. Success would be measured in the declining influence of pundits who are consistently wrong. Mark’s $500 pundit would be put out of business, and maybe too those pundits who predicted Hillary Clinton’s win or an easy and cheap U.S. victory in Iraq would fall out of our news feeds. "],
["persuasion-metrics.html", "2 Winning Over Public Opinion By Measuring Minds Changed Moving beyond focus groups and traffic data Analyzing the text of discussion forums to find mental shifts Avoiding ‘manipulation’ by staying within ethical boundaries", " 2 Winning Over Public Opinion By Measuring Minds Changed Keywords persuasion, sentiment analysis, natural language processing, big data, online forums Moving beyond focus groups and traffic data Imagine you are tasked with writing an article to convince Americans that climate change is real. What resources would you consult before writing, and after you hit publish how would you measure success? These questions are familiar and frustrating to anyone working in advocacy communications. Organizations with significant budgets can pay for framing research. Consultants compile a set of test messages and discuss them with focus groups. If the organization has even more money, they might conduct some polling. Aside from the expense, such tests take months. Determining the success of any persuasive content is tougher yet. Organizations look to pageviews, likes, and shares of their content for indicators of what connected. But engagement metrics don’t equal agreement and don’t reveal if the audience was already converted. Could we improve on this expensive guesswork for clients like foundations, think tanks, and political campaigns by actually observing people changing their minds in online discourse? Analyzing the text of discussion forums to find mental shifts I am proposing a tool that builds a database of online commenters who have opined on the client’s issue of interest. The database – containing commenters, their comments, a date, and contextual relationships – would be populated by searching discussion forums such as Reddit and Disqus through their APIs and feeds. Natural language processing techniques would be critical to identifying relevant discussions. Next, sentiment analysis would be conducted across the database, searching for cases where individual commenters appear to have shifted their attitude on the issue of interest. A shift could also be identified by a change in upvotes or likes on the topic. For those whose opinions shifted, the tool would look for the emergence of new phrases and keywords near the time of the attitude shift and attempt to match those back to an article or a comment in a thread. In this way, the tool would identify the text that persuaded. The client would receive the texts that persuaded, as well as analysis on those texts to find most common keywords and phrases, tones, and author characteristics. This analysis would be superior to framing research in that it draws ideas from far more people, it continually updates in real-time, it is based on observed change rather than self-reporting in an artificial focus-group, and it would eventually be cheaper to produce. The tool could also highlight mind-changing content published by the client or quoting their staff, thereby giving organizations a way to measure minds changed by their communication efforts. This, not engagement, is the true metric of interest. Avoiding ‘manipulation’ by staying within ethical boundaries The clients for this project – advocacy organizations – have nothing more precious than their reputation, so the project is designed to adhere to strong ethical standards. Specifically: The solution avoids experimenting with unsuspecting users in favor of observing their expressions on public fora. The data gathering does not rely on the client to push (or take away) messages in front of users as part of a secret experiment, as Facebook did with its controversial emotional contagion experiment. The algorithm would be listening to the wisdom of the crowds, uncovering the most persuasive arguments from both published writers and millions of commenters alike. This bottom-up approach shows respect for the notion of a democratic public square. The data gathering would be limited to public discussion forums where anonymity is greater rather than social networks where user identities and psychographics are more fully known. This should limit the ability of clients to veer into quick manipulation of Type 1 thinking and keep them focused on deliberative Type 2 thinking. The controversy over “fake news” and Russian social media advertising during the 2016 U.S. election highlights a growing public concern over unscrupulous efforts to manipulate opinion online. This project points to a way to improve persuasion that doesn’t rely on churning out deceptive copy, invading people’s privacy, or undermining democratic discourse. "]
]
